{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "rotary-newsletter",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorrect-longitude",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "import sklearn.model_selection\n",
    "import random\n",
    "from termcolor import colored\n",
    "from tensorflow.keras.layers import Input,Lambda,Dense,Flatten,MaxPool2D,Dropout,BatchNormalization,Activation,GlobalAveragePooling2D,LeakyReLU,subtract\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam,RMSprop\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow as tf\n",
    "from keras.regularizers import l2\n",
    "from keras.layers import concatenate\n",
    "import math\n",
    "from timeit import default_timer as timer\n",
    "from tensorflow.python.keras.utils.vis_utils import plot_model\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optimum-danish",
   "metadata": {},
   "source": [
    "# Data splitting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suited-metadata",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[]\n",
    "\n",
    "def create_data():\n",
    "\tfor category in categories:\n",
    "\t\tpath = os.path.join(DATADIR, category)\n",
    "\t\tclass_num=categories.index(category)\n",
    "\t\tfor img in os.listdir(path):\n",
    "\t\t\ttry:\n",
    "\t\t\t\timg_array = cv2.imread(os.path.join(path, img))\n",
    "\t\t\t\tdata.append([img_array,class_num])\n",
    "\t\t\texcept Exception as e:\n",
    "\t\t\t\tpass\n",
    "start = timer()\n",
    "create_data()\n",
    "dt = timer() - start\n",
    "print(\"Data collected in %f s\" % dt)\n",
    "\n",
    "\n",
    "\n",
    "random.shuffle(data)\n",
    "features=[]\n",
    "labels=[]\n",
    "\n",
    "for i,j in data:\n",
    "\tfeatures.append(i)\n",
    "\tlabels.append(j)\n",
    "\n",
    "\n",
    "features=np.array(features)\n",
    "labels=np.array(labels)\n",
    "\n",
    "print(colored(f\"Before reshaping: {features.shape}\",color=\"green\"))\n",
    "features=np.reshape(features,((-1,182,218,3)))\n",
    "print(colored(f\"After reshaping: {features.shape}\",color=\"green\"))\n",
    "\n",
    "\n",
    "\n",
    "np.save(\"features\",features)\n",
    "np.save(\"labels\",labels)\n",
    "\n",
    "features=np.load(\"features.npy\")\n",
    "labels=np.load(\"labels.npy\")\n",
    "\n",
    "\n",
    "data_train, data_test, target_train, target_test = sklearn.model_selection.train_test_split(features, labels,test_size=0.2)\n",
    "\n",
    "np.save(\"train_data_0.5\",data_train)\n",
    "np.save(\"test_data_0.5\",data_test)\n",
    "np.save(\"train_label_0.5\",target_train)\n",
    "np.save(\"test_label_0.5\",target_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parallel-liquid",
   "metadata": {},
   "source": [
    "# Loading data and normalizing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "western-multiple",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/CPU:0'):\n",
    "\n",
    "    X_train = np.load('//home//adwitiya123//Downloads//train_data.npy')\n",
    "    X_train = X_train/255\n",
    "    Y_train = np.load('//home//adwitiya123//Downloads//train_label.npy')\n",
    "\n",
    "    X_test = np.load('//home//adwitiya123//Downloads//test_data.npy')\n",
    "    X_test=X_test/255\n",
    "    Y_test = np.load('//home//adwitiya123//Downloads//test_label.npy')\n",
    "\n",
    "    np.save('X_train_normalized',X_train)\n",
    "    np.save('X_test_normalized',X_test)\n",
    "X_val = np.load('D:\\\\data\\\\data_first\\\\val_data.npy')\n",
    "Y_val = np.load('D:\\\\data\\\\data_first\\\\val_label.npy')\n",
    "\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dutch-annex",
   "metadata": {},
   "source": [
    "# VGG-16 model for Feature extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divine-greeting",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device('/CPU:0'):\n",
    "    \n",
    "    from tensorflow.keras.applications.vgg16 import VGG16\n",
    "    \n",
    "    vgg = VGG16(input_tensor = Input(shape = (182, 218, 3)), include_top = False, weights = 'imagenet')\n",
    "    \n",
    "    for layer in vgg.layers:\n",
    "        layer.trainable = False\n",
    "        \n",
    "    last_layer = vgg.get_layer('block5_pool')\n",
    "    last_output = last_layer.output\n",
    "    \n",
    "    features = Flatten()(last_output)\n",
    "    \n",
    "    model = Model(inputs = vgg.input, outputs = features, name = 'VGG16_FeatureExtractor')\n",
    "    plot_model(model, show_shapes = True, show_layer_names = True, to_file = 'feat.png')\n",
    "\n",
    "    X_train=np.load(\"X_train_normalized.npy\")\n",
    "    train_features = model.predict(X_train)\n",
    "    np.save('train_features_normalized_first.npy', train_features)\n",
    "    \n",
    "    X_test=np.load(\"X_test_normalized.npy\")\n",
    "    test_features = model.predict(X_test)\n",
    "    np.save('test_features_normalized_first.npy', test_features)\n",
    "    \n",
    "    val_features = model.predict(X_val)\n",
    "    np.save('val_features.npy', val_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finite-market",
   "metadata": {},
   "source": [
    "# Standardize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "constant-dealing",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features=np.load(\"test_features_normalized_first.npy\")\n",
    "train_features=np.load(\"train_features_normalized_first.npy\")\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_sc_train = scaler.fit_transform(train_features)\n",
    "X_sc_test = scaler.transform(test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "touched-terminal",
   "metadata": {},
   "source": [
    "# Finding components for PCA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-volleyball",
   "metadata": {},
   "outputs": [],
   "source": [
    "NCOMPONENTS = 4000\n",
    "\n",
    "pca = PCA(n_components = NCOMPONENTS)\n",
    "#pca.fit(X_sc_train)\n",
    "\n",
    "features_pca_train = pca.fit_transform(X_sc_train)\n",
    "features_pca_test = pca.transform(X_sc_test)\n",
    "#features_pca_val = pca.transform(X_sc_val)\n",
    "\n",
    "\n",
    "# plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "# plt.xlabel('Number of components')\n",
    "# plt.ylabel('Cumulative explained variance')\n",
    "\n",
    "\n",
    "\n",
    "# plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "\n",
    "# fig, ax = plt.subplots()\n",
    "# xi = np.arange(1, 15001, step = 1)\n",
    "# y = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "# plt.ylim(0.0,1.1)\n",
    "# plt.plot(xi, y, marker='o', linestyle='--', color='b')\n",
    "\n",
    "# plt.xlabel('Number of Components')\n",
    "# plt.xticks(np.arange(0, 15000, step = 1000)) #change from 0-based array index to 1-based human-readable label\n",
    "# plt.ylabel('Cumulative variance (%)')\n",
    "# plt.title('The number of components needed to explain variance')\n",
    "\n",
    "# plt.axhline(y=0.95, color='r', linestyle='-')\n",
    "# plt.text(0.5, 0.85, '95% cut-off threshold', color = 'red', fontsize=16)\n",
    "\n",
    "# ax.grid(axis='x')\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "np.save('features_pca_train_normalized.npy', features_pca_train)\n",
    "np.save('features_pca_test_normalized.npy', features_pca_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "christian-walter",
   "metadata": {},
   "source": [
    "# Creating pairs (1s and 0s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "covered-julian",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_pca_train=np.load('features_pca_train_normalized.npy')\n",
    "features_pca_test=np.load('features_pca_test_normalized.npy')\n",
    "Y_train = np.load('//home//adwitiya123//Downloads//train_label.npy')\n",
    "Y_test = np.load('//home//adwitiya123//Downloads//test_label.npy')\n",
    "Y_train = Y_train.astype('float32')\n",
    "Y_test = Y_test.astype('float32')\n",
    "\n",
    "def create_pairs(x, digit_indices):\n",
    "  pairs = []\n",
    "  labels = []\n",
    "   \n",
    "  n=min([len(digit_indices[d]) for d in range(4)]) -1\n",
    "   \n",
    "  for d in range(4):\n",
    "    for i in range(n):\n",
    "      z1, z2 = digit_indices[d][i], digit_indices[d][i+1]\n",
    "      pairs += [[x[z1], x[z2]]]\n",
    "      inc = random.randrange(1, 4)\n",
    "      dn = (d + inc) % 4\n",
    "      z1, z2 = digit_indices[d][i], digit_indices[dn][i]\n",
    "      pairs += [[x[z1], x[z2]]]\n",
    "      labels += [1,0]\n",
    "  return np.array(pairs), np.array(labels) \n",
    "\n",
    "digit_indices = [np.where(Y_train == i)[0] for i in range(4)]\n",
    "features_train_pca_pairs, features_train_pca_y = create_pairs(features_pca_train, digit_indices) \n",
    "\n",
    "digit_indices = [np.where(Y_test == i)[0] for i in range(4)]\n",
    "features_test_pca_pairs, features_test_pca_y = create_pairs(features_pca_test, digit_indices) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "drawn-league",
   "metadata": {},
   "source": [
    "# Loading dataset for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equipped-family",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairTrain=np.load(\"features_pca_train_pairs_normalized.npy\")\n",
    "labelTrain=np.load(\"features_pca_train_y_norm.npy\")\n",
    "labelTrain=labelTrain.astype('float32')\n",
    "\n",
    "pairTest=np.load(\"features_pca_test_pairs_normalized.npy\")\n",
    "labelTest=np.load(\"features_pca_test_y_norm.npy\")\n",
    "labelTest=labelTest.astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defensive-reader",
   "metadata": {},
   "source": [
    "# Dimension check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "everyday-canadian",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"pair train:{pairTrain.shape}\")\n",
    "print(f\"labeltrain:{labelTrain.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
